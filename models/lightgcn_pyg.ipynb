{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lxoenua1-u5r"
   },
   "outputs": [],
   "source": [
    "# pip install torch_geometric\n",
    "\n",
    "# Optional dependencies:\n",
    "# pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.3.0+cu118.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWkNAZvtNQ6R"
   },
   "source": [
    "# Implementing a Recommender System using LightGCN\n",
    "\n",
    "In this colab, we explain how to set up a graph recommender system using the [LighGCN](https://arxiv.org/abs/2002.02126) model. Specifically, we apply LightGCN to a movie recommendation task using [PyTorch](https://pytorch.org/) and [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/).\n",
    "\n",
    "We use the [MovieLens](https://grouplens.org/datasets/movielens/) (*small*) dataset which has 100,000 ratings applied to 9,000 movies by 600 users.\n",
    "\n",
    "Our implementation was inspired by the following documentation and repositories:\n",
    "- https://github.com/gusye1234/LightGCN-PyTorch\n",
    "- https://www.kaggle.com/dipanjandas96/lightgcn-pytorch-from-scratch\n",
    "- https://pytorch-geometric.readthedocs.io/en/latest/notes/load_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5OBSacx6Q03m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "from torch_geometric.utils import structured_negative_sampling\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import Adj\n",
    "\n",
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mZ6-zPxPklE"
   },
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "We load the dataset and set ratings >=4 on a 0.5 ~ 5 scale as an edge between users and movies.\n",
    "\n",
    "We split the edges of the graph using a 80/10/10 train/validation/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cRC_IazQ4Oj",
    "outputId": "efad15fe-b6d5-4dde-cbe0-a1d3cdb5c5a2"
   },
   "outputs": [],
   "source": [
    "# # download the dataset\n",
    "# url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "# extract_zip(download_url(url, '.'), '.')\n",
    "\n",
    "# movie_path = './ml-latest-small/movies.csv'\n",
    "# rating_path = './ml-latest-small/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"data\\foodcom-recipes-and-reviews\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "od.download_kaggle_dataset(\"https://www.kaggle.com/datasets/irkaal/foodcom-recipes-and-reviews\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_path = \"data/foodcom-recipes-and-reviews/recipes.csv\"\n",
    "reviews_path = \"data/foodcom-recipes-and-reviews/reviews.csv\"\n",
    "recipes = pd.read_parquet(\"data/foodcom-recipes-and-reviews/recipes.parquet\")\n",
    "reviews = pd.read_parquet(\"data/foodcom-recipes-and-reviews/reviews.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ReviewId', 'RecipeId', 'AuthorId', 'AuthorName', 'Rating', 'Review',\n",
       "       'DateSubmitted', 'DateModified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['RecipeId', 'Name', 'AuthorId', 'AuthorName', 'CookTime', 'PrepTime',\n",
       "       'TotalTime', 'DatePublished', 'Description', 'Images', 'RecipeCategory',\n",
       "       'Keywords', 'RecipeIngredientQuantities', 'RecipeIngredientParts',\n",
       "       'AggregatedRating', 'ReviewCount', 'Calories', 'FatContent',\n",
       "       'SaturatedFatContent', 'CholesterolContent', 'SodiumContent',\n",
       "       'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent',\n",
       "       'RecipeServings', 'RecipeYield', 'RecipeInstructions'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay algunos valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecipeId faltantes en recipes.csv: {424301, 371545, 432898, 194165}\n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv(reviews_path)\n",
    "recipes = pd.read_csv(recipes_path)\n",
    "\n",
    "missing_recipe_ids = set(reviews['RecipeId']) - set(recipes['RecipeId'])\n",
    "print(f\"RecipeId faltantes en recipes.csv: {missing_recipe_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o2P3zYR8Q8EX"
   },
   "outputs": [],
   "source": [
    "def load_node_csv(path, index_col):\n",
    "    \"\"\"Loads csv containing node information.\n",
    "\n",
    "    Args:\n",
    "        path (str): path to csv file\n",
    "        index_col (str): column name of index column\n",
    "\n",
    "    Returns:\n",
    "        dict: mapping of csv row to node id\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, index_col=index_col)\n",
    "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
    "    return mapping\n",
    "\n",
    "\n",
    "recipes_df = pd.read_csv(recipes_path)\n",
    "recipes_mapping = load_node_csv(recipes_path, index_col=\"RecipeId\")\n",
    "\n",
    "reviews_df = pd.read_csv(reviews_path)\n",
    "filtered_reviews_df = reviews_df[reviews_df[\"RecipeId\"].isin(recipes_mapping.keys())]\n",
    "\n",
    "user_mapping = {author_id: idx for idx, author_id in enumerate(filtered_reviews_df[\"AuthorId\"].unique())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkJzQlxSRDEq"
   },
   "outputs": [],
   "source": [
    "# load edges between users and movies\n",
    "def load_edge_csv(path, src_index_col, src_mapping, dst_index_col, dst_mapping, link_index_col, rating_threshold=4):\n",
    "    \"\"\"Loads csv containing edges between users and items\n",
    "\n",
    "    Args:\n",
    "        path (str): path to csv file\n",
    "        src_index_col (str): column name of users\n",
    "        src_mapping (dict): mapping between row number and user id\n",
    "        dst_index_col (str): column name of items\n",
    "        dst_mapping (dict): mapping between row number and item id\n",
    "        link_index_col (str): column name of user item interaction\n",
    "        rating_threshold (int, optional): Threshold to determine positivity of edge. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 2 by N matrix containing the node ids of N user-item edges\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    edge_index = None\n",
    "\n",
    "    valid_mask = df[dst_index_col].isin(dst_mapping.keys())\n",
    "    filtered_df = df[valid_mask]\n",
    "\n",
    "    src = [src_mapping[index] for index in filtered_df[src_index_col]]\n",
    "    dst = [dst_mapping[index] for index in filtered_df[dst_index_col]]\n",
    "\n",
    "    edge_attr = torch.from_numpy(filtered_df[link_index_col].values).view(-1, 1).to(torch.long) >= rating_threshold\n",
    "\n",
    "\n",
    "    edge_index = [[], []]\n",
    "    for i in range(edge_attr.shape[0]):\n",
    "        if edge_attr[i]:\n",
    "            edge_index[0].append(src[i])\n",
    "            edge_index[1].append(dst[i])\n",
    "\n",
    "    return torch.tensor(edge_index)\n",
    "\n",
    "\n",
    "edge_index = load_edge_csv(\n",
    "    reviews_path,\n",
    "    src_index_col='AuthorId',\n",
    "    src_mapping=user_mapping,\n",
    "    dst_index_col='RecipeId',\n",
    "    dst_mapping=recipes_mapping,\n",
    "    link_index_col='Rating',\n",
    "    rating_threshold=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wIueYZfaT6_H"
   },
   "outputs": [],
   "source": [
    "# split the edges of the graph using a 80/10/10 train/validation/test split\n",
    "num_users, num_movies = len(user_mapping), len(recipes_mapping)\n",
    "num_interactions = edge_index.shape[1]\n",
    "all_indices = [i for i in range(num_interactions)]\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    all_indices, test_size=0.2, random_state=1)\n",
    "val_indices, test_indices = train_test_split(\n",
    "    test_indices, test_size=0.5, random_state=1)\n",
    "\n",
    "train_edge_index = edge_index[:, train_indices]\n",
    "val_edge_index = edge_index[:, val_indices]\n",
    "test_edge_index = edge_index[:, test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U5yKILBJUAN6"
   },
   "outputs": [],
   "source": [
    "# convert edge indices into Sparse Tensors: https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
    "train_sparse_edge_index = SparseTensor(row=train_edge_index[0], col=train_edge_index[1], sparse_sizes=(\n",
    "    num_users + num_movies, num_users + num_movies))\n",
    "val_sparse_edge_index = SparseTensor(row=val_edge_index[0], col=val_edge_index[1], sparse_sizes=(\n",
    "    num_users + num_movies, num_users + num_movies))\n",
    "test_sparse_edge_index = SparseTensor(row=test_edge_index[0], col=test_edge_index[1], sparse_sizes=(\n",
    "    num_users + num_movies, num_users + num_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KqKI1VduKcwf"
   },
   "outputs": [],
   "source": [
    "# function which random samples a mini-batch of positive and negative samples\n",
    "def sample_mini_batch(batch_size, edge_index):\n",
    "    \"\"\"Randomly samples indices of a minibatch given an adjacency matrix\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): minibatch size\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        tuple: user indices, positive item indices, negative item indices\n",
    "    \"\"\"\n",
    "    edges = structured_negative_sampling(edge_index)\n",
    "    edges = torch.stack(edges, dim=0)\n",
    "    indices = random.choices(\n",
    "        [i for i in range(edges[0].shape[0])], k=batch_size)\n",
    "    batch = edges[:, indices]\n",
    "    user_indices, pos_item_indices, neg_item_indices = batch[0], batch[1], batch[2]\n",
    "    return user_indices, pos_item_indices, neg_item_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOB5kDmtUrUY"
   },
   "source": [
    "# Implementing LightGCN\n",
    "\n",
    "## Light Graph Convolution\n",
    "Between each layer, LightGCN uses the following propagation rule for user and item embeddings.\n",
    "\n",
    "\\begin{equation}\n",
    "e_u^{(k+1)} = \\sum_{i \\in N_u} \\frac{1}{\\sqrt{|N_u|}\\sqrt{|N_i|}} e_i^{(k)} \\quad e_i^{(k+1)} = \\sum_{u \\in N_i} \\frac{1}{\\sqrt{|N_i|}\\sqrt{|N_u|}} e_u^{(k)}\n",
    "\\end{equation}\n",
    "\n",
    "$N_u$: the set of all neighbors of user $u$ (items liked by $u$)\n",
    "\n",
    "$N_i$: the set of all neighbors of item $i$ (users who liked $i$)\n",
    "\n",
    "$e_u^{(k)}$ : k-th layer user embedding\n",
    "\n",
    "$e_i^{(k)}$ : k-th layer item embedding\n",
    "\n",
    "\n",
    "\n",
    "## Layer Combination and Model Prediction\n",
    "The only trainable parameters of LightGCN are the 0-th layer embeddings $e_u^{(0)}$ and $e_i^{(0)}$ for each user and item. We combine the embeddings obtained at each layer of propagation to form the final embeddings for all user and item, $e_u$ and $e_i$ via the follwing equation.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "e_u = \\sum_{k = 0}^K \\alpha_k e_u^{(k)} \\quad e_i = \\sum_{k = 0}^K \\alpha_k e_i^{(k)}\n",
    "\\end{equation}\n",
    "\n",
    "$\\alpha_k$ : hyperparameter which weights the contribution of the k-th layer embedding to the final embedding\n",
    "\n",
    "The model prediction is obtained by taking the inner product of the final user and item embeddings.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y}_{ui} = e_u^Te_i\n",
    "\\end{equation}\n",
    "\n",
    "## Matrix Form\n",
    "In our implementation, we utilize the matrix form of LightGCN. We perform multi-scale diffusion to obtain the final embedding, which sums embeddings diffused across multi-hop scales.\n",
    "\n",
    "\\begin{equation}\n",
    "E^{(K)} = \\alpha_0 E^{(0)} + \\alpha_1 \\tilde{A}^1 E^{(0)} + \\alpha_2 \\tilde{A}^2 E^{(0)} + \\cdot \\cdot \\cdot + \\alpha_K \\tilde{A}^K \\tilde{A} E^{(0)}\n",
    "\\end{equation}\n",
    "\n",
    "$E^{(0)} \\in \\mathcal{R}^{(M + N)} \\times T$ : stacked initial item and user embeddings where $M$, $N$, and $T$ denote the number of users, number of items, and the dimension of each embedding respectively\n",
    "\n",
    "$\\tilde{A} = D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$ : symmetrically normalized adjacency matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o9GvYg9ehDOX"
   },
   "outputs": [],
   "source": [
    "# defines LightGCN model\n",
    "class LightGCN(MessagePassing):\n",
    "    \"\"\"LightGCN Model as proposed in https://arxiv.org/abs/2002.02126\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, K=3, add_self_loops=False):\n",
    "        \"\"\"Initializes LightGCN Model\n",
    "\n",
    "        Args:\n",
    "            num_users (int): Number of users\n",
    "            num_items (int): Number of items\n",
    "            embedding_dim (int, optional): Dimensionality of embeddings. Defaults to 8.\n",
    "            K (int, optional): Number of message passing layers. Defaults to 3.\n",
    "            add_self_loops (bool, optional): Whether to add self loops for message passing. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_users, self.num_items = num_users, num_items\n",
    "        self.embedding_dim, self.K = embedding_dim, K\n",
    "        self.add_self_loops = add_self_loops\n",
    "\n",
    "        self.users_emb = nn.Embedding(\n",
    "            num_embeddings=self.num_users, embedding_dim=self.embedding_dim) # e_u^0\n",
    "        self.items_emb = nn.Embedding(\n",
    "            num_embeddings=self.num_items, embedding_dim=self.embedding_dim) # e_i^0\n",
    "\n",
    "        nn.init.normal_(self.users_emb.weight, std=0.1)\n",
    "        nn.init.normal_(self.items_emb.weight, std=0.1)\n",
    "\n",
    "    def forward(self, edge_index: SparseTensor):\n",
    "        \"\"\"Forward propagation of LightGCN Model.\n",
    "\n",
    "        Args:\n",
    "            edge_index (SparseTensor): adjacency matrix\n",
    "\n",
    "        Returns:\n",
    "            tuple (Tensor): e_u_k, e_u_0, e_i_k, e_i_0\n",
    "        \"\"\"\n",
    "        # compute \\tilde{A}: symmetrically normalized adjacency matrix\n",
    "        edge_index_norm = gcn_norm(\n",
    "            edge_index, add_self_loops=self.add_self_loops)\n",
    "\n",
    "        emb_0 = torch.cat([self.users_emb.weight, self.items_emb.weight]) # E^0\n",
    "        embs = [emb_0]\n",
    "        emb_k = emb_0\n",
    "\n",
    "        # multi-scale diffusion\n",
    "        for i in range(self.K):\n",
    "            emb_k = self.propagate(edge_index_norm, x=emb_k)\n",
    "            embs.append(emb_k)\n",
    "\n",
    "        embs = torch.stack(embs, dim=1)\n",
    "        emb_final = torch.mean(embs, dim=1) # E^K\n",
    "\n",
    "        users_emb_final, items_emb_final = torch.split(\n",
    "            emb_final, [self.num_users, self.num_items]) # splits into e_u^K and e_i^K\n",
    "\n",
    "        # returns e_u^K, e_u^0, e_i^K, e_i^0\n",
    "        return users_emb_final, self.users_emb.weight, items_emb_final, self.items_emb.weight\n",
    "\n",
    "    def message(self, x_j: Tensor) -> Tensor:\n",
    "        return x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        # computes \\tilde{A} @ x\n",
    "        return matmul(adj_t, x)\n",
    "\n",
    "model = LightGCN(num_users, num_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My8eqloiBccE"
   },
   "source": [
    "# Loss Function\n",
    "\n",
    "\n",
    "\n",
    "We utilize a Bayesian Personalized Ranking (BPR) loss, a pairwise objective which encourages the predictions of positive samples to be higher than negative samples for each user.\n",
    "\n",
    "\\begin{equation}\n",
    "L_{BPR} = -\\sum_{u = 1}^M \\sum_{i \\in N_u} \\sum_{j \\notin N_u} \\ln{\\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})} + \\lambda ||E^{(0)}||^2\n",
    "\\end{equation}\n",
    "\n",
    "$\\hat{y}_{u}$: predicted score of a positive sample\n",
    "\n",
    "$\\hat{y}_{uj}$: predicted score of a negative sample\n",
    "\n",
    "$\\lambda$: hyperparameter which controls the L2 regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QmPs1xS-BYfe"
   },
   "outputs": [],
   "source": [
    "def bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0, lambda_val):\n",
    "    \"\"\"Bayesian Personalized Ranking Loss as described in https://arxiv.org/abs/1205.2618\n",
    "\n",
    "    Args:\n",
    "        users_emb_final (torch.Tensor): e_u_k\n",
    "        users_emb_0 (torch.Tensor): e_u_0\n",
    "        pos_items_emb_final (torch.Tensor): positive e_i_k\n",
    "        pos_items_emb_0 (torch.Tensor): positive e_i_0\n",
    "        neg_items_emb_final (torch.Tensor): negative e_i_k\n",
    "        neg_items_emb_0 (torch.Tensor): negative e_i_0\n",
    "        lambda_val (float): lambda value for regularization loss term\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: scalar bpr loss value\n",
    "    \"\"\"\n",
    "    reg_loss = lambda_val * (users_emb_0.norm(2).pow(2) +\n",
    "                             pos_items_emb_0.norm(2).pow(2) +\n",
    "                             neg_items_emb_0.norm(2).pow(2)) # L2 loss\n",
    "\n",
    "    pos_scores = torch.mul(users_emb_final, pos_items_emb_final)\n",
    "    pos_scores = torch.sum(pos_scores, dim=-1) # predicted scores of positive samples\n",
    "    neg_scores = torch.mul(users_emb_final, neg_items_emb_final)\n",
    "    neg_scores = torch.sum(neg_scores, dim=-1) # predicted scores of negative samples\n",
    "\n",
    "    loss = -torch.mean(torch.nn.functional.softplus(pos_scores - neg_scores)) + reg_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CS7HVr3qLQGx"
   },
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We evalaluate our model using the following metrics\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Recall} = \\frac{TP}{TP + FP}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Precision} = \\frac{TP}{TP + FN}\n",
    "\\end{equation}\n",
    "\n",
    "**Dicounted Cumulative Gain (DCG)** at rank position p is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{DCG}_\\text{p} = \\sum_{i = 1}^p \\frac{2^{rel_i} - 1}{\\log_2{(i + 1)}}\n",
    "\\end{equation}\n",
    "\n",
    "p: a particular rank position\n",
    "\n",
    "$rel_i \\in \\{0, 1\\}$ : graded relevance of the result at position $i$\n",
    "\n",
    "**Idealised Dicounted Cumulative Gain (IDCG)**, namely the maximum possible DCG, at rank position $p$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{IDCG}_\\text{p} = \\sum_{i = 1}^{|REL_p|} \\frac{2^{rel_i} - 1}{\\log_2{(i + 1)}}\n",
    "\\end{equation}\n",
    "\n",
    "$|REL_p|$ : list of items ordered by their relevance up to position p\n",
    "\n",
    "**Normalized Dicounted Cumulative Gain (NDCG)** at rank position $p$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{nDCG}_\\text{p} = \\frac{\\text{DCG}_p}{\\text{nDCG}_p}\n",
    "\\end{equation}\n",
    "\n",
    "Specifically, we use the metrics recall@K, precision@K, and NDCG@K. @K indicates that these metrics are computed on the top K recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nHO2gdhRSwzJ"
   },
   "outputs": [],
   "source": [
    "# helper function to get N_u\n",
    "def get_user_positive_items(edge_index):\n",
    "    \"\"\"Generates dictionary of positive items for each user\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of positive items for each user\n",
    "    \"\"\"\n",
    "    user_pos_items = {}\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        user = edge_index[0][i].item()\n",
    "        item = edge_index[1][i].item()\n",
    "        if user not in user_pos_items:\n",
    "            user_pos_items[user] = []\n",
    "        user_pos_items[user].append(item)\n",
    "    return user_pos_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8We4BTtfS4NV"
   },
   "outputs": [],
   "source": [
    "# computes recall@K and precision@K\n",
    "def RecallPrecision_ATk(groundTruth, r, k):\n",
    "    \"\"\"Computers recall @ k and precision @ k\n",
    "\n",
    "    Args:\n",
    "        groundTruth (list): list of lists containing highly rated items of each user\n",
    "        r (list): list of lists indicating whether each top k item recommended to each user\n",
    "            is a top k ground truth item or not\n",
    "        k (intg): determines the top k items to compute precision and recall on\n",
    "\n",
    "    Returns:\n",
    "        tuple: recall @ k, precision @ k\n",
    "    \"\"\"\n",
    "    num_correct_pred = torch.sum(r, dim=-1)  # number of correctly predicted items per user\n",
    "    # number of items liked by each user in the test set\n",
    "    user_num_liked = torch.Tensor([len(groundTruth[i])\n",
    "                                  for i in range(len(groundTruth))])\n",
    "    recall = torch.mean(num_correct_pred / user_num_liked)\n",
    "    precision = torch.mean(num_correct_pred) / k\n",
    "    return recall.item(), precision.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9v4A3Ek4TE02"
   },
   "outputs": [],
   "source": [
    "# computes NDCG@K\n",
    "def NDCGatK_r(groundTruth, r, k):\n",
    "    \"\"\"Computes Normalized Discounted Cumulative Gain (NDCG) @ k\n",
    "\n",
    "    Args:\n",
    "        groundTruth (list): list of lists containing highly rated items of each user\n",
    "        r (list): list of lists indicating whether each top k item recommended to each user\n",
    "            is a top k ground truth item or not\n",
    "        k (int): determines the top k items to compute ndcg on\n",
    "\n",
    "    Returns:\n",
    "        float: ndcg @ k\n",
    "    \"\"\"\n",
    "    assert len(r) == len(groundTruth)\n",
    "\n",
    "    test_matrix = torch.zeros((len(r), k))\n",
    "\n",
    "    for i, items in enumerate(groundTruth):\n",
    "        length = min(len(items), k)\n",
    "        test_matrix[i, :length] = 1\n",
    "    max_r = test_matrix\n",
    "    idcg = torch.sum(max_r * 1. / torch.log2(torch.arange(2, k + 2)), axis=1)\n",
    "    dcg = r * (1. / torch.log2(torch.arange(2, k + 2)))\n",
    "    dcg = torch.sum(dcg, axis=1)\n",
    "    idcg[idcg == 0.] = 1.\n",
    "    ndcg = dcg / idcg\n",
    "    ndcg[torch.isnan(ndcg)] = 0.\n",
    "    return torch.mean(ndcg).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "x6_741LlTMwI"
   },
   "outputs": [],
   "source": [
    "# wrapper function to get evaluation metrics\n",
    "def get_metrics(model, edge_index, exclude_edge_indices, k):\n",
    "    \"\"\"Computes the evaluation metrics: recall, precision, and ndcg @ k\n",
    "\n",
    "    Args:\n",
    "        model (LighGCN): lightgcn model\n",
    "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
    "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
    "        k (int): determines the top k items to compute metrics on\n",
    "\n",
    "    Returns:\n",
    "        tuple: recall @ k, precision @ k, ndcg @ k\n",
    "    \"\"\"\n",
    "    user_embedding = model.users_emb.weight\n",
    "    item_embedding = model.items_emb.weight\n",
    "\n",
    "    # get ratings between every user and item - shape is num users x num movies\n",
    "    rating = torch.matmul(user_embedding, item_embedding.T)\n",
    "\n",
    "    for exclude_edge_index in exclude_edge_indices:\n",
    "        # gets all the positive items for each user from the edge index\n",
    "        user_pos_items = get_user_positive_items(exclude_edge_index)\n",
    "        # get coordinates of all edges to exclude\n",
    "        exclude_users = []\n",
    "        exclude_items = []\n",
    "        for user, items in user_pos_items.items():\n",
    "            exclude_users.extend([user] * len(items))\n",
    "            exclude_items.extend(items)\n",
    "\n",
    "        # set ratings of excluded edges to large negative value\n",
    "        rating[exclude_users, exclude_items] = -(1 << 10)\n",
    "\n",
    "    # get the top k recommended items for each user\n",
    "    _, top_K_items = torch.topk(rating, k=k)\n",
    "\n",
    "    # get all unique users in evaluated split\n",
    "    users = edge_index[0].unique()\n",
    "\n",
    "    test_user_pos_items = get_user_positive_items(edge_index)\n",
    "\n",
    "    # convert test user pos items dictionary into a list\n",
    "    test_user_pos_items_list = [\n",
    "        test_user_pos_items[user.item()] for user in users]\n",
    "\n",
    "    # determine the correctness of topk predictions\n",
    "    r = []\n",
    "    for user in users:\n",
    "        ground_truth_items = test_user_pos_items[user.item()]\n",
    "        label = list(map(lambda x: x in ground_truth_items, top_K_items[user]))\n",
    "        r.append(label)\n",
    "    r = torch.Tensor(np.array(r).astype('float'))\n",
    "\n",
    "    recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, k)\n",
    "    ndcg = NDCGatK_r(test_user_pos_items_list, r, k)\n",
    "\n",
    "    return recall, precision, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_batchwise(model, edge_index, exclude_edge_indices, k, batch_size=1000):\n",
    "    \"\"\"Computes recall, precision, and ndcg @ k in batches to reduce memory usage.\n",
    "    \n",
    "    Args:\n",
    "        model (LightGCN): LightGCN model\n",
    "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
    "        exclude_edge_indices ([torch.Tensor]): list of 2 by N tensors to exclude from evaluation\n",
    "        k (int): top-k items\n",
    "        batch_size (int): size of user/item batches to process at once\n",
    "    \n",
    "    Returns:\n",
    "        tuple: recall @ k, precision @ k, ndcg @ k\n",
    "    \"\"\"\n",
    "    user_embedding = model.users_emb.weight\n",
    "    item_embedding = model.items_emb.weight\n",
    "    num_users = user_embedding.shape[0]\n",
    "    \n",
    "    # Create a tensor to hold cumulative results\n",
    "    recall_accum, precision_accum, ndcg_accum = 0, 0, 0\n",
    "    num_users_evaluated = 0\n",
    "\n",
    "    # Process users in batches\n",
    "    for start in tqdm(range(0, num_users, batch_size)):\n",
    "        end = min(start + batch_size, num_users)\n",
    "        user_batch = user_embedding[start:end]\n",
    "        \n",
    "        # Compute batch-wise ratings\n",
    "        rating_batch = torch.matmul(user_batch, item_embedding.T)\n",
    "        \n",
    "        # Exclude already interacted items for users in the batch\n",
    "        for exclude_edge_index in exclude_edge_indices:\n",
    "            user_pos_items = get_user_positive_items(exclude_edge_index)\n",
    "            for user_idx in range(start, end):\n",
    "                user_id = user_idx - start\n",
    "                if user_idx in user_pos_items:\n",
    "                    exclude_items = user_pos_items[user_idx]\n",
    "                    rating_batch[user_id, exclude_items] = -(1 << 10)\n",
    "        # Top-K items for the batch\n",
    "        _, top_K_items = torch.topk(rating_batch, k=k)\n",
    "\n",
    "        # Evaluate metrics for the batch\n",
    "        users = edge_index[0].unique()\n",
    "        test_user_pos_items = get_user_positive_items(edge_index)\n",
    "        batch_test_user_pos_items = [\n",
    "            test_user_pos_items[user.item()] for user in users[start:end]]\n",
    "        \n",
    "        r = []\n",
    "        for user_id, user in enumerate(users[start:end]):\n",
    "            ground_truth_items = test_user_pos_items[user.item()]\n",
    "            label = list(map(lambda x: x in ground_truth_items, top_K_items[user_id]))\n",
    "            r.append(label)\n",
    "        r = torch.Tensor(np.array(r).astype('float'))\n",
    "        \n",
    "        if len(batch_test_user_pos_items) == 0:\n",
    "            continue\n",
    "        # Calculate recall, precision, and ndcg for the batch\n",
    "        batch_recall, batch_precision = RecallPrecision_ATk(batch_test_user_pos_items, r, k)\n",
    "        batch_ndcg = NDCGatK_r(batch_test_user_pos_items, r, k)\n",
    "\n",
    "        # Accumulate results\n",
    "        recall_accum += batch_recall * len(batch_test_user_pos_items)\n",
    "        precision_accum += batch_precision * len(batch_test_user_pos_items)\n",
    "        ndcg_accum += batch_ndcg * len(batch_test_user_pos_items)\n",
    "        num_users_evaluated += len(batch_test_user_pos_items)\n",
    "    \n",
    "    # Normalize accumulated metrics\n",
    "    recall = recall_accum / num_users_evaluated\n",
    "    precision = precision_accum / num_users_evaluated\n",
    "    ndcg = ndcg_accum / num_users_evaluated\n",
    "\n",
    "    return recall, precision, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yr_qESXASsVw"
   },
   "outputs": [],
   "source": [
    "# wrapper function to evaluate model\n",
    "def evaluation(model, edge_index, sparse_edge_index, exclude_edge_indices, k, lambda_val):\n",
    "    \"\"\"Evaluates model loss and metrics including recall, precision, ndcg @ k\n",
    "\n",
    "    Args:\n",
    "        model (LighGCN): lightgcn model\n",
    "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
    "        sparse_edge_index (sparseTensor): sparse adjacency matrix for split to evaluate\n",
    "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
    "        k (int): determines the top k items to compute metrics on\n",
    "        lambda_val (float): determines lambda for bpr loss\n",
    "\n",
    "    Returns:\n",
    "        tuple: bpr loss, recall @ k, precision @ k, ndcg @ k\n",
    "    \"\"\"\n",
    "    # get embeddings\n",
    "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(\n",
    "        sparse_edge_index)\n",
    "    edges = structured_negative_sampling(\n",
    "        edge_index, contains_neg_self_loops=False)\n",
    "    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n",
    "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
    "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[\n",
    "        pos_item_indices], items_emb_0[pos_item_indices]\n",
    "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[\n",
    "        neg_item_indices], items_emb_0[neg_item_indices]\n",
    "\n",
    "    loss = bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0,\n",
    "                    neg_items_emb_final, neg_items_emb_0, lambda_val).item()\n",
    "\n",
    "    recall, precision, ndcg = get_metrics(\n",
    "        model, edge_index, exclude_edge_indices, k)\n",
    "    \n",
    "    return loss, recall, precision, ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYw1cUgPTjws"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MQL2W-NQTeFd"
   },
   "outputs": [],
   "source": [
    "# define contants\n",
    "ITERATIONS = 30000\n",
    "BATCH_SIZE = 1024\n",
    "LR = 1e-3\n",
    "ITERS_PER_EVAL = 1000\n",
    "ITERS_PER_LR_DECAY = 200\n",
    "K = 10\n",
    "LAMBDA = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49JDkBtKTfE-",
    "outputId": "0a62891e-4a65-4b8c-c5d1-1316281b50c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda.\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}.\")\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "edge_index = edge_index.to(device)\n",
    "train_edge_index = train_edge_index.to(device)\n",
    "train_sparse_edge_index = train_sparse_edge_index.to(device)\n",
    "\n",
    "val_edge_index = val_edge_index.to(device)\n",
    "val_sparse_edge_index = val_sparse_edge_index.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYjrDp1w-hiP",
    "outputId": "ce7f5102-2090-44a0-8245-42fa30853679"
   },
   "outputs": [],
   "source": [
    "Training = False\n",
    "# training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "if Training:\n",
    "    for iter in tqdm(range(ITERATIONS)):\n",
    "        # forward propagation\n",
    "        users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(\n",
    "            train_sparse_edge_index)\n",
    "\n",
    "        # mini batching\n",
    "        user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(\n",
    "            BATCH_SIZE, train_edge_index)\n",
    "        user_indices, pos_item_indices, neg_item_indices = user_indices.to(\n",
    "            device), pos_item_indices.to(device), neg_item_indices.to(device)\n",
    "        users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
    "        pos_items_emb_final, pos_items_emb_0 = items_emb_final[\n",
    "            pos_item_indices], items_emb_0[pos_item_indices]\n",
    "        neg_items_emb_final, neg_items_emb_0 = items_emb_final[\n",
    "            neg_item_indices], items_emb_0[neg_item_indices]\n",
    "\n",
    "        # loss computation\n",
    "        train_loss = bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final,\n",
    "                            pos_items_emb_0, neg_items_emb_final, neg_items_emb_0, LAMBDA)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if iter % ITERS_PER_EVAL == 0:\n",
    "            model.eval()\n",
    "            # val_loss, recall, precision, ndcg = evaluation(\n",
    "            #     model, val_edge_index, val_sparse_edge_index, [train_edge_index], K, LAMBDA)\n",
    "            # print(f\"[Iteration {iter}/{ITERATIONS}] train_loss: {round(train_loss.item(), 5)}, val_loss: {round(val_loss, 5)}, val_recall@{K}: {round(recall, 5)}, val_precision@{K}: {round(precision, 5)}, val_ndcg@{K}: {round(ndcg, 5)}\")\n",
    "            train_losses.append(train_loss.item())\n",
    "            # val_losses.append(val_loss)\n",
    "            model.train()\n",
    "\n",
    "        if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar y cargar\n",
    "load = True\n",
    "save = False\n",
    "\n",
    "model_save_path = \"lightgcn_model.pth\"\n",
    "\n",
    "if load:\n",
    "    model = LightGCN(num_users, num_movies)\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    model = model.to(device)\n",
    "\n",
    "if save:\n",
    "    torch.save(model.state_dict(), model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "nLcdvV5iXBSv",
    "outputId": "b4059176-24b0-46b5-ea65-68c2f9afd6d2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAHHCAYAAACvJxw8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2xElEQVR4nO3de1xVVf7/8fcBBEQEvCCIopRaXtPUUMzGJigsKy1NJfP29ZtdvOSofdPR1GzMzPJeOn2/U46Wk1qNlpmFqN0kL2jmvbJU1IDMABUFgvX7ox9nOoJLRG7HXs/HYz/yrL3W3p+9ZOQ9e69zjsMYYwQAAIAieVR0AQAAAJUZYQkAAMCCsAQAAGBBWAIAALAgLAEAAFgQlgAAACwISwAAABaEJQAAAAvCEgAAgAVhCcAVGzRokCIiIko0dsqUKXI4HKVbUDFdSd0A/jgIS8BVzOFwFGvbtGlTRZcKAJWWg++GA65eb7zxhsvrJUuWKD4+XkuXLnVpv/322xUSElLi8+Tm5io/P18+Pj6XPfbXX3/Vr7/+Kl9f3xKfv6QGDRqkTZs26fDhw+V+bgDuw6uiCwBQdh566CGX119++aXi4+MLtV8oKytLfn5+xT5PlSpVSlSfJHl5ecnLi3+KytPZs2dVrVq1ii4DcBs8hgP+4G699Va1bNlSSUlJ+tOf/iQ/Pz/99a9/lSStXr1a3bp1U1hYmHx8fNSoUSM9++yzysvLcznGhWt/Dh8+LIfDoRdffFGvvvqqGjVqJB8fH910003atm2by9ii1iw5HA4NHz5cq1atUsuWLeXj46MWLVpo3bp1herftGmT2rdvL19fXzVq1Eh///vfr2gd1NmzZzVmzBiFh4fLx8dH119/vV588UVdeBM+Pj5enTt3VlBQkPz9/XX99dc7563A/Pnz1aJFC/n5+alGjRpq3769li1bdskazp8/rylTpui6666Tr6+v6tatq/vvv1+HDh1yXnNRj08L5n3x4sXOtkGDBsnf31+HDh3SXXfdperVq6tfv34aPny4/P39lZWVVej8cXFxCg0Ndfl7/vDDD3XLLbeoWrVqql69urp166a9e/e6jEtJSdHgwYNVv359+fj4qG7duurevTt37uD2+L9zAPTzzz/rzjvvVN++ffXQQw85H8ktXrxY/v7+Gj16tPz9/bVhwwZNmjRJmZmZmjlz5iWPu2zZMp0+fVqPPPKIHA6HXnjhBd1///36/vvvL3k36vPPP9e7776rxx9/XNWrV9e8efPUs2dPHT16VLVq1ZIk7dy5U127dlXdunX1zDPPKC8vT1OnTlVwcHCJ5sEYo3vvvVcbN27UkCFD1KZNG3300Ud68skndfz4cc2ePVuStHfvXt1999264YYbNHXqVPn4+Oi7777TF1984TzW//7v/2rkyJHq1auXnnjiCZ0/f15ff/21tmzZogcffPCiNeTl5enuu+9WQkKC+vbtqyeeeEKnT59WfHy89uzZo0aNGl32df3666+KjY1V586d9eKLL8rPz08RERF6+eWX9cEHH+iBBx5w9s3KytL777+vQYMGydPTU5K0dOlSDRw4ULGxsZoxY4aysrK0cOFCde7cWTt37nQG5Z49e2rv3r0aMWKEIiIilJaWpvj4eB09epSF9HBvBsAfxrBhw8yF/7Pv0qWLkWQWLVpUqH9WVlahtkceecT4+fmZ8+fPO9sGDhxoGjZs6Hz9ww8/GEmmVq1a5tSpU8721atXG0nm/fffd7ZNnjy5UE2SjLe3t/nuu++cbbt27TKSzPz5851t99xzj/Hz8zPHjx93tn377bfGy8ur0DGLcmHdq1atMpLM3/72N5d+vXr1Mg6Hw1nP7NmzjSTz008/XfTY3bt3Ny1atLhkDRd67bXXjCQza9asQvvy8/ONMcZs3LjRSDIbN2502V8w76+//rqzbeDAgUaSGTduXKFj1atXz/Ts2dOlfcWKFUaS+fTTT40xxpw+fdoEBQWZhx9+2KVfSkqKCQwMdLb/8ssvRpKZOXPmZV8zUNnxGA6AfHx8NHjw4ELtVatWdf759OnTOnnypG655RZlZWXpwIEDlzxunz59VKNGDefrW265RZL0/fffX3JsTEyMy12UG264QQEBAc6xeXl5Wr9+vXr06KGwsDBnv8aNG+vOO++85PGLsnbtWnl6emrkyJEu7WPGjJExRh9++KEkKSgoSNJvjynz8/OLPFZQUJCOHTtW6LHjpbzzzjuqXbu2RowYUWjflXzEwmOPPVboWA888IDWrl2rM2fOONuXL1+uevXqqXPnzpJ+e9yYnp6uuLg4nTx50rl5enqqQ4cO2rhxo6Tffla8vb21adMm/fLLLyWuE6iMCEsAVK9ePXl7exdq37t3r+677z4FBgYqICBAwcHBzsXhGRkZlzxugwYNXF4XBKfi/DK9cGzB+IKxaWlpOnfunBo3blyoX1FtxXHkyBGFhYWpevXqLu3NmjVz7pd+C4E333yz/vu//1shISHq27evVqxY4RKcnnrqKfn7+ysyMlJNmjTRsGHDXB7TXcyhQ4d0/fXXl+qidy8vL9WvX79Qe58+fXTu3Dm99957kqQzZ85o7dq1euCBB5zB7Ntvv5Uk3XbbbQoODnbZPv74Y6WlpUn6LXDPmDFDH374oUJCQvSnP/1JL7zwglJSUkrtOoCKQlgC4HIHqUB6erq6dOmiXbt2aerUqXr//fcVHx+vGTNmSNJF76j8XsGalwuZYnxiyZWMLWtVq1bVp59+qvXr16t///76+uuv1adPH91+++3ORdHNmjXTwYMH9dZbb6lz585655131LlzZ02ePPmKz3+xO0wXLrwv4OPjIw+Pwv/cd+zYUREREVqxYoUk6f3339e5c+fUp08fZ5+Cv+elS5cqPj6+0LZ69Wpn31GjRumbb77R9OnT5evrq6efflrNmjXTzp07S3ytQGVAWAJQpE2bNunnn3/W4sWL9cQTT+juu+9WTEyMy2O1ilSnTh35+vrqu+++K7SvqLbiaNiwoU6cOKHTp0+7tBc8cmzYsKGzzcPDQ9HR0Zo1a5b27dunadOmacOGDc7HUpJUrVo19enTR6+//rqOHj2qbt26adq0aTp//vxFa2jUqJEOHjyo3Nzci/Yp+DtIT093aS+483U5evfurXXr1ikzM1PLly9XRESEOnbs6FKP9Nt8x8TEFNpuvfXWQvWPGTNGH3/8sfbs2aOcnBy99NJLl10XUJkQlgAUqeDOzu/v5OTk5OiVV16pqJJceHp6KiYmRqtWrdKJEyec7d99951zbdHluuuuu5SXl6cFCxa4tM+ePVsOh8O5FurUqVOFxrZp00aSlJ2dLem3dxj+nre3t5o3by5jjDUI9ezZUydPnixUg/Sfv4uGDRvK09NTn376qcv+kvzd9OnTR9nZ2frnP/+pdevWqXfv3i77Y2NjFRAQoOeee67Iun/66SdJv72L7sIQ2KhRI1WvXt05J4C74qMDABSpU6dOqlGjhgYOHKiRI0fK4XBo6dKlleIxWIEpU6bo448/1s0336zHHnvMGXRatmypr7766rKPd8899+jPf/6zJkyYoMOHD6t169b6+OOPtXr1ao0aNcp5l2Xq1Kn69NNP1a1bNzVs2FBpaWl65ZVXVL9+fefC6DvuuEOhoaG6+eabFRISov3792vBggXq1q1boTVRvzdgwAAtWbJEo0eP1tatW3XLLbfo7NmzWr9+vR5//HF1795dgYGBeuCBBzR//nw5HA41atRIa9asca4fuhxt27ZV48aNNWHCBGVnZ7s8gpOkgIAALVy4UP3791fbtm3Vt29fBQcH6+jRo/rggw908803a8GCBfrmm28UHR2t3r17q3nz5vLy8tK///1vpaamqm/fvpddF1CZEJYAFKlWrVpas2aNxowZo4kTJ6pGjRp66KGHFB0drdjY2IouT5LUrl07ffjhhxo7dqyefvpphYeHa+rUqdq/f3+x3q13IQ8PD7333nuaNGmSli9frtdff10RERGaOXOmxowZ4+x377336vDhw3rttdd08uRJ1a5dW126dNEzzzyjwMBASdIjjzyiN998U7NmzdKZM2dUv359jRw5UhMnTrTW4OnpqbVr12ratGlatmyZ3nnnHdWqVUudO3dWq1atnP3mz5+v3NxcLVq0SD4+Purdu7dmzpypli1bXvZ19+nTR9OmTVPjxo3Vtm3bQvsffPBBhYWF6fnnn9fMmTOVnZ2tevXq6ZZbbnG+izI8PFxxcXFKSEjQ0qVL5eXlpaZNm2rFihXq2bPnZdcEVCZ8NxyAq06PHj20d+9e5zu5AOBKsGYJgFs7d+6cy+tvv/1Wa9euLbTwGABKijtLANxa3bp1NWjQIF177bU6cuSIFi5cqOzsbO3cuVNNmjSp6PIAXAVYswTArXXt2lX/+te/lJKSIh8fH0VFRem5554jKAEoNdxZAgAAsGDNEgAAgAVhCQAAwII1S6UgPz9fJ06cUPXq1a/oW8EBAED5Mcbo9OnTCgsLK/L7EwsQlkrBiRMnFB4eXtFlAACAEkhOTlb9+vUvup+wVAoKvrogOTlZAQEBFVwNAAAojszMTIWHh1u/gkgiLJWKgkdvAQEBhCUAANzMpZbQsMAbAADAgrAEAABgQVgCAACwYM0SAACVVF5ennJzcyu6DLdVpUoVeXp6XvFxCEsAAFQyxhilpKQoPT29oktxe0FBQQoNDb2iz0EkLAEAUMkUBKU6derIz8+PDzwuAWOMsrKylJaWJkmqW7duiY9FWAIAoBLJy8tzBqVatWpVdDlurWrVqpKktLQ01alTp8SP5FjgDQBAJVKwRsnPz6+CK7k6FMzjlaz9IiwBAFAJ8eitdJTGPBKWAAAALAhLAACg0oqIiNCcOXMqtAbCEgAAuGIOh8O6TZkypUTH3bZtm4YOHVq6xV4m3g0HAACu2I8//uj88/LlyzVp0iQdPHjQ2ebv7+/8szFGeXl58vK6dAwJDg4u3UJLgDtLAADgioWGhjq3wMBAORwO5+sDBw6oevXq+vDDD9WuXTv5+Pjo888/16FDh9S9e3eFhITI399fN910k9avX+9y3AsfwzkcDv3f//2f7rvvPvn5+alJkyZ67733yvTaCEsAAFRyxhhl5fxa7psxplSvY9y4cXr++ee1f/9+3XDDDTpz5ozuuusuJSQkaOfOneratavuueceHT161HqcZ555Rr1799bXX3+tu+66S/369dOpU6dKtdbf4zEcAACV3LncPDWf9FG5n3ff1Fj5eZdeVJg6dapuv/125+uaNWuqdevWztfPPvus/v3vf+u9997T8OHDL3qcQYMGKS4uTpL03HPPad68edq6dau6du1aarX+HneWAABAuWjfvr3L6zNnzmjs2LFq1qyZgoKC5O/vr/3791/yztINN9zg/HO1atUUEBDg/FqTssCdJQAAKrmqVTy1b2pshZy3NFWrVs3l9dixYxUfH68XX3xRjRs3VtWqVdWrVy/l5ORYj1OlShWX1w6HQ/n5+aVa6+8RlgAAqOQcDkepPg6rLL744gsNGjRI9913n6Tf7jQdPny4YosqAo/hAABAhWjSpIneffddffXVV9q1a5cefPDBMr1DVFKEJQAAUCFmzZqlGjVqqFOnTrrnnnsUGxurtm3bVnRZhThMab8v8A8oMzNTgYGBysjIUEBAQEWXAwBwY+fPn9cPP/yga665Rr6+vhVdjtuzzWdxf39zZwkAAMCCsAQAAGBBWAIAALAgLAEAAFgQlgAAqIR4/1XpKI15JCwBAFCJFHw6dVZWVgVXcnUomMcLP/X7clx9HwcKAIAb8/T0VFBQkPO7zvz8/ORwOCq4KvdjjFFWVpbS0tIUFBQkT8+Sf3ULYQkAgEomNDRUksr0y2H/KIKCgpzzWVKEJQAAKhmHw6G6deuqTp06ys3Nrehy3FaVKlWu6I5SAcISAACVlKenZ6n8sseVYYE3AACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsCEsAAAAWhCUAAAALwhIAAIAFYQkAAMCCsAQAAGDhdmHp5ZdfVkREhHx9fdWhQwdt3brV2n/lypVq2rSpfH191apVK61du/aifR999FE5HA7NmTOnlKsGAADuyq3C0vLlyzV69GhNnjxZO3bsUOvWrRUbG6u0tLQi+2/evFlxcXEaMmSIdu7cqR49eqhHjx7as2dPob7//ve/9eWXXyosLKysLwMAALgRtwpLs2bN0sMPP6zBgwerefPmWrRokfz8/PTaa68V2X/u3Lnq2rWrnnzySTVr1kzPPvus2rZtqwULFrj0O378uEaMGKE333xTVapUKY9LAQAAbsJtwlJOTo6SkpIUExPjbPPw8FBMTIwSExOLHJOYmOjSX5JiY2Nd+ufn56t///568skn1aJFi7IpHgAAuC2vii6guE6ePKm8vDyFhIS4tIeEhOjAgQNFjklJSSmyf0pKivP1jBkz5OXlpZEjRxa7luzsbGVnZztfZ2ZmFnssAABwL25zZ6ksJCUlae7cuVq8eLEcDkexx02fPl2BgYHOLTw8vAyrBAAAFcltwlLt2rXl6emp1NRUl/bU1FSFhoYWOSY0NNTa/7PPPlNaWpoaNGggLy8veXl56ciRIxozZowiIiIuWsv48eOVkZHh3JKTk6/s4gAAQKXlNmHJ29tb7dq1U0JCgrMtPz9fCQkJioqKKnJMVFSUS39Jio+Pd/bv37+/vv76a3311VfOLSwsTE8++aQ++uiji9bi4+OjgIAAlw0AAFyd3GbNkiSNHj1aAwcOVPv27RUZGak5c+bo7NmzGjx4sCRpwIABqlevnqZPny5JeuKJJ9SlSxe99NJL6tatm9566y1t375dr776qiSpVq1aqlWrlss5qlSpotDQUF1//fXle3EAAKBScquw1KdPH/3000+aNGmSUlJS1KZNG61bt865iPvo0aPy8PjPzbJOnTpp2bJlmjhxov7617+qSZMmWrVqlVq2bFlRlwAAANyMwxhjKroId5eZmanAwEBlZGTwSA4AADdR3N/fbrNmCQAAoCIQlgAAACwISwAAABaEJQAAAAvCEgAAgAVhCQAAwIKwBAAAYEFYAgAAsCAsAQAAWBCWAAAALAhLAAAAFoQlAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsCEsAAAAWhCUAAAALwhIAAIAFYQkAAMCCsAQAAGBBWAIAALAgLAEAAFgQlgAAACwISwAAABaEJQAAAAvCEgAAgAVhCQAAwIKwBAAAYEFYAgAAsCAsAQAAWBCWAAAALAhLAAAAFoQlAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsCEsAAAAWhCUAAAALwhIAAIAFYQkAAMCCsAQAAGBBWAIAALAgLAEAAFgQlgAAACwISwAAABaEJQAAAAvCEgAAgAVhCQAAwIKwBAAAYEFYAgAAsHC7sPTyyy8rIiJCvr6+6tChg7Zu3Wrtv3LlSjVt2lS+vr5q1aqV1q5d69yXm5urp556Sq1atVK1atUUFhamAQMG6MSJE2V9GQAAwE24VVhavny5Ro8ercmTJ2vHjh1q3bq1YmNjlZaWVmT/zZs3Ky4uTkOGDNHOnTvVo0cP9ejRQ3v27JEkZWVlaceOHXr66ae1Y8cOvfvuuzp48KDuvffe8rwsAABQiTmMMaaiiyiuDh066KabbtKCBQskSfn5+QoPD9eIESM0bty4Qv379Omjs2fPas2aNc62jh07qk2bNlq0aFGR59i2bZsiIyN15MgRNWjQoFh1ZWZmKjAwUBkZGQoICCjBlQEAgPJW3N/fbnNnKScnR0lJSYqJiXG2eXh4KCYmRomJiUWOSUxMdOkvSbGxsRftL0kZGRlyOBwKCgoqlboBAIB786roAorr5MmTysvLU0hIiEt7SEiIDhw4UOSYlJSUIvunpKQU2f/8+fN66qmnFBcXZ02Y2dnZys7Odr7OzMws7mUAAAA34zZ3lspabm6uevfuLWOMFi5caO07ffp0BQYGOrfw8PByqhIAAJQ3twlLtWvXlqenp1JTU13aU1NTFRoaWuSY0NDQYvUvCEpHjhxRfHz8JdcdjR8/XhkZGc4tOTm5BFcEAADcgduEJW9vb7Vr104JCQnOtvz8fCUkJCgqKqrIMVFRUS79JSk+Pt6lf0FQ+vbbb7V+/XrVqlXrkrX4+PgoICDAZQMAAFcnt1mzJEmjR4/WwIED1b59e0VGRmrOnDk6e/asBg8eLEkaMGCA6tWrp+nTp0uSnnjiCXXp0kUvvfSSunXrprfeekvbt2/Xq6++Kum3oNSrVy/t2LFDa9asUV5ennM9U82aNeXt7V0xFwoAACoNtwpLffr00U8//aRJkyYpJSVFbdq00bp165yLuI8ePSoPj//cLOvUqZOWLVumiRMn6q9//auaNGmiVatWqWXLlpKk48eP67333pMktWnTxuVcGzdu1K233lou1wUAACovt/qcpcqKz1kCAMD9XHWfswQAAFARCEsAAAAWhCUAAAALwhIAAIAFYQkAAMCCsAQAAGBBWAIAALAgLAEAAFgQlgAAACwISwAAABaEJQAAAAvCEgAAgAVhCQAAwIKwBAAAYEFYAgAAsCAsAQAAWBCWAAAALAhLAAAAFoQlAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsCEsAAAAWhCUAAAALwhIAAIAFYQkAAMCCsAQAAGBBWAIAALAgLAEAAFgQlgAAACwISwAAABaEJQAAAAvCEgAAgAVhCQAAwIKwBAAAYEFYAgAAsCAsAQAAWBCWAAAALAhLAAAAFoQlAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsShSWkpOTdezYMefrrVu3atSoUXr11VdLrTAAAIDKoERh6cEHH9TGjRslSSkpKbr99tu1detWTZgwQVOnTi3VAgEAACpSicLSnj17FBkZKUlasWKFWrZsqc2bN+vNN9/U4sWLS7M+AACAClWisJSbmysfHx9J0vr163XvvfdKkpo2baoff/yx9KoDAACoYCUKSy1atNCiRYv02WefKT4+Xl27dpUknThxQrVq1SrVAgEAACpSicLSjBkz9Pe//1233nqr4uLi1Lp1a0nSe++953w8BwAAcDVwGGNMSQbm5eUpMzNTNWrUcLYdPnxYfn5+qlOnTqkV6A4yMzMVGBiojIwMBQQEVHQ5AACgGIr7+7tEd5bOnTun7OxsZ1A6cuSI5syZo4MHD/7hghIAALi6lSgsde/eXUuWLJEkpaenq0OHDnrppZfUo0cPLVy4sFQLvNDLL7+siIgI+fr6qkOHDtq6dau1/8qVK9W0aVP5+vqqVatWWrt2rct+Y4wmTZqkunXrqmrVqoqJidG3335blpcAAADcSInC0o4dO3TLLbdIkt5++22FhIToyJEjWrJkiebNm1eqBf7e8uXLNXr0aE2ePFk7duxQ69atFRsbq7S0tCL7b968WXFxcRoyZIh27typHj16qEePHtqzZ4+zzwsvvKB58+Zp0aJF2rJli6pVq6bY2FidP3++zK4DAAC4jxKtWfLz89OBAwfUoEED9e7dWy1atNDkyZOVnJys66+/XllZWWVRqzp06KCbbrpJCxYskCTl5+crPDxcI0aM0Lhx4wr179Onj86ePas1a9Y42zp27Kg2bdpo0aJFMsYoLCxMY8aM0dixYyVJGRkZCgkJ0eLFi9W3b99i1cWaJQAA3E+Zrllq3LixVq1apeTkZH300Ue64447JElpaWllFhZycnKUlJSkmJgYZ5uHh4diYmKUmJhY5JjExESX/pIUGxvr7P/DDz8oJSXFpU9gYKA6dOhw0WNKUnZ2tjIzM102AABwdSpRWJo0aZLGjh2riIgIRUZGKioqSpL08ccf68YbbyzVAgucPHlSeXl5CgkJcWkPCQlRSkpKkWNSUlKs/Qv+eznHlKTp06crMDDQuYWHh1/29QAAAPdQorDUq1cvHT16VNu3b9dHH33kbI+Ojtbs2bNLrbjKavz48crIyHBuycnJFV0SAAAoI14lHRgaGqrQ0FAdO3ZMklS/fv0y/UDK2rVry9PTU6mpqS7tqampCg0NvWiNtv4F/01NTVXdunVd+rRp0+aitfj4+Di/7gUAAFzdSnRnKT8/X1OnTlVgYKAaNmyohg0bKigoSM8++6zy8/NLu0ZJkre3t9q1a6eEhASXOhISEpyPAS8UFRXl0l+S4uPjnf2vueYahYaGuvTJzMzUli1bLnpMAADwx1KiO0sTJkzQP/7xDz3//PO6+eabJUmff/65pkyZovPnz2vatGmlWmSB0aNHa+DAgWrfvr0iIyM1Z84cnT17VoMHD5YkDRgwQPXq1dP06dMlSU888YS6dOmil156Sd26ddNbb72l7du369VXX5UkORwOjRo1Sn/729/UpEkTXXPNNXr66acVFhamHj16lMk1AAAAN2NKoG7dumb16tWF2letWmXCwsJKcshimz9/vmnQoIHx9vY2kZGR5ssvv3Tu69Klixk4cKBL/xUrVpjrrrvOeHt7mxYtWpgPPvjAZX9+fr55+umnTUhIiPHx8THR0dHm4MGDl1VTRkaGkWQyMjJKfF0AAKB8Fff3d4k+Z8nX11dff/21rrvuOpf2gwcPqk2bNjp37lwpRTn3wOcsAQDgfsr0c5Zat27t/GDI31uwYIFuuOGGkhwSAACgUirRmqUXXnhB3bp10/r1650LoRMTE5WcnFzou9cAAADcWYnuLHXp0kXffPON7rvvPqWnpys9PV3333+/9u7dq6VLl5Z2jQAAABWmRGuWLmbXrl1q27at8vLySuuQboE1SwAAuJ8yXbMEAADwR0FYAgAAsCAsAQAAWFzWu+Huv/9+6/709PQrqQUAAKDSuaywFBgYeMn9AwYMuKKCAAAAKpPLCkuvv/56WdUBAABQKbFmCQAAwIKwBAAAYEFYAgAAsCAsAQAAWBCWAAAALAhLAAAAFoQlAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsCEsAAAAWhCUAAAALwhIAAIAFYQkAAMCCsAQAAGBBWAIAALAgLAEAAFgQlgAAACwISwAAABaEJQAAAAvCEgAAgAVhCQAAwIKwBAAAYEFYAgAAsCAsAQAAWBCWAAAALAhLAAAAFoQlAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsCEsAAAAWhCUAAAALwhIAAIAFYQkAAMCCsAQAAGBBWAIAALAgLAEAAFgQlgAAACwISwAAABZuE5ZOnTqlfv36KSAgQEFBQRoyZIjOnDljHXP+/HkNGzZMtWrVkr+/v3r27KnU1FTn/l27dikuLk7h4eGqWrWqmjVrprlz55b1pQAAADfiNmGpX79+2rt3r+Lj47VmzRp9+umnGjp0qHXMX/7yF73//vtauXKlPvnkE504cUL333+/c39SUpLq1KmjN954Q3v37tWECRM0fvx4LViwoKwvBwAAuAmHMcZUdBGXsn//fjVv3lzbtm1T+/btJUnr1q3TXXfdpWPHjiksLKzQmIyMDAUHB2vZsmXq1auXJOnAgQNq1qyZEhMT1bFjxyLPNWzYMO3fv18bNmwodn2ZmZkKDAxURkaGAgICSnCFAACgvBX397db3FlKTExUUFCQMyhJUkxMjDw8PLRly5YixyQlJSk3N1cxMTHOtqZNm6pBgwZKTEy86LkyMjJUs2ZNaz3Z2dnKzMx02QAAwNXJLcJSSkqK6tSp49Lm5eWlmjVrKiUl5aJjvL29FRQU5NIeEhJy0TGbN2/W8uXLL/l4b/r06QoMDHRu4eHhxb8YAADgVio0LI0bN04Oh8O6HThwoFxq2bNnj7p3767JkyfrjjvusPYdP368MjIynFtycnK51AgAAMqfV0WefMyYMRo0aJC1z7XXXqvQ0FClpaW5tP/66686deqUQkNDixwXGhqqnJwcpaenu9xdSk1NLTRm3759io6O1tChQzVx4sRL1u3j4yMfH59L9gMAAO6vQsNScHCwgoODL9kvKipK6enpSkpKUrt27SRJGzZsUH5+vjp06FDkmHbt2qlKlSpKSEhQz549JUkHDx7U0aNHFRUV5ey3d+9e3XbbbRo4cKCmTZtWClcFAACuJm7xbjhJuvPOO5WamqpFixYpNzdXgwcPVvv27bVs2TJJ0vHjxxUdHa0lS5YoMjJSkvTYY49p7dq1Wrx4sQICAjRixAhJv61Nkn579HbbbbcpNjZWM2fOdJ7L09OzWCGuAO+GAwDA/RT393eF3lm6HG+++aaGDx+u6OhoeXh4qGfPnpo3b55zf25urg4ePKisrCxn2+zZs519s7OzFRsbq1deecW5/+2339ZPP/2kN954Q2+88YazvWHDhjp8+HC5XBcAAKjc3ObOUmXGnSUAANzPVfU5SwAAABWFsAQAAGBBWAIAALAgLAEAAFgQlgAAACwISwAAABaEJQAAAAvCEgAAgAVhCQAAwIKwBAAAYEFYAgAAsCAsAQAAWBCWAAAALAhLAAAAFoQlAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsCEsAAAAWhCUAAAALwhIAAIAFYQkAAMCCsAQAAGBBWAIAALAgLAEAAFgQlgAAACwISwAAABaEJQAAAAvCEgAAgAVhCQAAwIKwBAAAYEFYAgAAsCAsAQAAWBCWAAAALAhLAAAAFoQlAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsCEsAAAAWhCUAAAALwhIAAIAFYQkAAMCCsAQAAGBBWAIAALAgLAEAAFgQlgAAACwISwAAABaEJQAAAAvCEgAAgIXbhKVTp06pX79+CggIUFBQkIYMGaIzZ85Yx5w/f17Dhg1TrVq15O/vr549eyo1NbXIvj///LPq168vh8Oh9PT0MrgCAADgjtwmLPXr10979+5VfHy81qxZo08//VRDhw61jvnLX/6i999/XytXrtQnn3yiEydO6P777y+y75AhQ3TDDTeURekAAMCNOYwxpqKLuJT9+/erefPm2rZtm9q3by9JWrdune666y4dO3ZMYWFhhcZkZGQoODhYy5YtU69evSRJBw4cULNmzZSYmKiOHTs6+y5cuFDLly/XpEmTFB0drV9++UVBQUHFri8zM1OBgYHKyMhQQEDAlV0sAAAoF8X9/e0Wd5YSExMVFBTkDEqSFBMTIw8PD23ZsqXIMUlJScrNzVVMTIyzrWnTpmrQoIESExOdbfv27dPUqVO1ZMkSeXgUbzqys7OVmZnpsgEAgKuTW4SllJQU1alTx6XNy8tLNWvWVEpKykXHeHt7F7pDFBIS4hyTnZ2tuLg4zZw5Uw0aNCh2PdOnT1dgYKBzCw8Pv7wLAgAAbqNCw9K4cePkcDis24EDB8rs/OPHj1ezZs300EMPXfa4jIwM55acnFxGFQIAgIrmVZEnHzNmjAYNGmTtc+211yo0NFRpaWku7b/++qtOnTql0NDQIseFhoYqJydH6enpLneXUlNTnWM2bNig3bt36+2335YkFSzfql27tiZMmKBnnnmmyGP7+PjIx8enOJcIAADcXIWGpeDgYAUHB1+yX1RUlNLT05WUlKR27dpJ+i3o5Ofnq0OHDkWOadeunapUqaKEhAT17NlTknTw4EEdPXpUUVFRkqR33nlH586dc47Ztm2b/uu//kufffaZGjVqdKWXBwAArgIVGpaKq1mzZuratasefvhhLVq0SLm5uRo+fLj69u3rfCfc8ePHFR0drSVLligyMlKBgYEaMmSIRo8erZo1ayogIEAjRoxQVFSU851wFwaikydPOs93Oe+GAwAAVy+3CEuS9Oabb2r48OGKjo6Wh4eHevbsqXnz5jn35+bm6uDBg8rKynK2zZ4929k3OztbsbGxeuWVVyqifAAA4Kbc4nOWKjs+ZwkAAPdzVX3OEgAAQEUhLAEAAFgQlgAAACwISwAAABaEJQAAAAvCEgAAgAVhCQAAwIKwBAAAYEFYAgAAsCAsAQAAWBCWAAAALAhLAAAAFoQlAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsCEsAAAAWhCUAAAALwhIAAIAFYQkAAMCCsAQAAGBBWAIAALAgLAEAAFgQlgAAACwISwAAABaEJQAAAAvCEgAAgAVhCQAAwIKwBAAAYEFYAgAAsCAsAQAAWBCWAAAALAhLAAAAFoQlAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsvCq6gKuBMUaSlJmZWcGVAACA4ir4vV3we/xiCEul4PTp05Kk8PDwCq4EAABcrtOnTyswMPCi+x3mUnEKl5Sfn68TJ06oevXqcjgcFV1OhcrMzFR4eLiSk5MVEBBQ0eVctZjn8sNclw/muXwwz66MMTp9+rTCwsLk4XHxlUncWSoFHh4eql+/fkWXUakEBATwP8RywDyXH+a6fDDP5YN5/g/bHaUCLPAGAACwICwBAABYEJZQqnx8fDR58mT5+PhUdClXNea5/DDX5YN5Lh/Mc8mwwBsAAMCCO0sAAAAWhCUAAAALwhIAAIAFYQkAAMCCsITLdurUKfXr108BAQEKCgrSkCFDdObMGeuY8+fPa9iwYapVq5b8/f3Vs2dPpaamFtn3559/Vv369eVwOJSenl4GV+AeymKed+3apbi4OIWHh6tq1apq1qyZ5s6dW9aXUqm8/PLLioiIkK+vrzp06KCtW7da+69cuVJNmzaVr6+vWrVqpbVr17rsN8Zo0qRJqlu3rqpWraqYmBh9++23ZXkJbqE05zk3N1dPPfWUWrVqpWrVqiksLEwDBgzQiRMnyvoyKr3S/nn+vUcffVQOh0Nz5swp5ardkAEuU9euXU3r1q3Nl19+aT777DPTuHFjExcXZx3z6KOPmvDwcJOQkGC2b99uOnbsaDp16lRk3+7du5s777zTSDK//PJLGVyBeyiLef7HP/5hRo4caTZt2mQOHTpkli5daqpWrWrmz59f1pdTKbz11lvG29vbvPbaa2bv3r3m4YcfNkFBQSY1NbXI/l988YXx9PQ0L7zwgtm3b5+ZOHGiqVKlitm9e7ezz/PPP28CAwPNqlWrzK5du8y9995rrrnmGnPu3LnyuqxKp7TnOT093cTExJjly5ebAwcOmMTERBMZGWnatWtXnpdV6ZTFz3OBd99917Ru3dqEhYWZ2bNnl/GVVH6EJVyWffv2GUlm27ZtzrYPP/zQOBwOc/z48SLHpKenmypVqpiVK1c62/bv328kmcTERJe+r7zyiunSpYtJSEj4Q4elsp7n33v88cfNn//859IrvhKLjIw0w4YNc77Oy8szYWFhZvr06UX27927t+nWrZtLW4cOHcwjjzxijDEmPz/fhIaGmpkzZzr3p6enGx8fH/Ovf/2rDK7APZT2PBdl69atRpI5cuRI6RTthspqno8dO2bq1atn9uzZYxo2bEhYMsbwGA6XJTExUUFBQWrfvr2zLSYmRh4eHtqyZUuRY5KSkpSbm6uYmBhnW9OmTdWgQQMlJiY62/bt26epU6dqyZIl1i80/CMoy3m+UEZGhmrWrFl6xVdSOTk5SkpKcpkfDw8PxcTEXHR+EhMTXfpLUmxsrLP/Dz/8oJSUFJc+gYGB6tChg3XOr2ZlMc9FycjIkMPhUFBQUKnU7W7Kap7z8/PVv39/Pfnkk2rRokXZFO+G/ti/kXDZUlJSVKdOHZc2Ly8v1axZUykpKRcd4+3tXegftZCQEOeY7OxsxcXFaebMmWrQoEGZ1O5OymqeL7R582YtX75cQ4cOLZW6K7OTJ08qLy9PISEhLu22+UlJSbH2L/jv5RzzalcW83yh8+fP66mnnlJcXNwf9stgy2qeZ8yYIS8vL40cObL0i3ZjhCVIksaNGyeHw2HdDhw4UGbnHz9+vJo1a6aHHnqozM5RGVT0PP/enj171L17d02ePFl33HFHuZwTuFK5ubnq3bu3jDFauHBhRZdzVUlKStLcuXO1ePFiORyOii6nUvGq6AJQOYwZM0aDBg2y9rn22msVGhqqtLQ0l/Zff/1Vp06dUmhoaJHjQkNDlZOTo/T0dJe7Hqmpqc4xGzZs0O7du/X2229L+u0dRpJUu3ZtTZgwQc8880wJr6xyqeh5LrBv3z5FR0dr6NChmjhxYomuxd3Url1bnp6ehd6FWdT8FAgNDbX2L/hvamqq6tat69KnTZs2pVi9+yiLeS5QEJSOHDmiDRs2/GHvKkllM8+fffaZ0tLSXO7u5+XlacyYMZozZ44OHz5cuhfhTip60RTcS8HC4+3btzvbPvroo2ItPH777bedbQcOHHBZePzdd9+Z3bt3O7fXXnvNSDKbN2++6Ds7rmZlNc/GGLNnzx5Tp04d8+STT5bdBVRSkZGRZvjw4c7XeXl5pl69etYFsXfffbdLW1RUVKEF3i+++KJzf0ZGBgu8S3mejTEmJyfH9OjRw7Ro0cKkpaWVTeFuprTn+eTJky7/Du/evduEhYWZp556yhw4cKDsLsQNEJZw2bp27WpuvPFGs2XLFvP555+bJk2auLyl/dixY+b66683W7ZscbY9+uijpkGDBmbDhg1m+/btJioqykRFRV30HBs3bvxDvxvOmLKZ5927d5vg4GDz0EMPmR9//NG5/VF++bz11lvGx8fHLF682Ozbt88MHTrUBAUFmZSUFGOMMf379zfjxo1z9v/iiy+Ml5eXefHFF83+/fvN5MmTi/zogKCgILN69Wrz9ddfm+7du/PRAaU8zzk5Oebee+819evXN1999ZXLz252dnaFXGNlUBY/zxfi3XC/ISzhsv38888mLi7O+Pv7m4CAADN48GBz+vRp5/4ffvjBSDIbN250tp07d848/vjjpkaNGsbPz8/cd9995scff7zoOQhLZTPPkydPNpIKbQ0bNizHK6tY8+fPNw0aNDDe3t4mMjLSfPnll859Xbp0MQMHDnTpv2LFCnPdddcZb29v06JFC/PBBx+47M/PzzdPP/20CQkJMT4+PiY6OtocPHiwPC6lUivNeS74WS9q+/3P/x9Raf88X4iw9BuHMf9/cQgAAAAK4d1wAAAAFoQlAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYA4ApFRERozpw5FV0GgDJCWALgVgYNGqQePXpIkm699VaNGjWq3M69ePFily8pLrBt2zYNHTq03OoAUL68KroAAKhoOTk58vb2LvH44ODgUqwGQGXDnSUAbmnQoEH65JNPNHfuXDkcDjkcDh0+fFiStGfPHt15553y9/dXSEiI+vfvr5MnTzrH3nrrrRo+fLhGjRql2rVrKzY2VpI0a9YstWrVStWqVVN4eLgef/xxnTlzRpK0adMmDR48WBkZGc7zTZkyRVLhx3BHjx5V9+7d5e/vr4CAAPXu3VupqanO/VOmTFGbNm20dOlSRUREKDAwUH379tXp06fLdtIAlAhhCYBbmjt3rqKiovTwww/rxx9/1I8//qjw8HClp6frtttu04033qjt27dr3bp1Sk1NVe/evV3G//Of/5S3t7e++OILLVq0SJLk4eGhefPmae/evfrnP/+pDRs26H/+538kSZ06ddKcOXMUEBDgPN/YsWML1ZWfn6/u3bvr1KlT+uSTTxQfH6/vv/9effr0cel36NAhrVq1SmvWrNGaNWv0ySef6Pnnny+j2QJwJXgMB8AtBQYGytvbW35+fgoNDXW2L1iwQDfeeKOee+45Z9trr72m8PBwffPNN7ruuuskSU2aNNELL7zgcszfr3+KiIjQ3/72Nz366KN65ZVX5O3trcDAQDkcDpfzXSghIUG7d+/WDz/8oPDwcEnSkiVL1KJFC23btk033XSTpN9C1eLFi1W9enVJUv/+/ZWQkKBp06Zd2cQAKHXcWQJwVdm1a5c2btwof39/59a0aVNJv93NKdCuXbtCY9evX6/o6GjVq1dP1atXV//+/fXzzz8rKyur2Offv3+/wsPDnUFJkpo3b66goCDt37/f2RYREeEMSpJUt25dpaWlXda1Aigf3FkCcFU5c+aM7rnnHs2YMaPQvrp16zr/XK1aNZd9hw8f1t13363HHntM06ZNU82aNfX5559ryJAhysnJkZ+fX6nWWaVKFZfXDodD+fn5pXoOAKWDsATAbXl7eysvL8+lrW3btnrnnXcUEREhL6/i/xOXlJSk/Px8vfTSS/Lw+O2m+4oVKy55vgs1a9ZMycnJSk5Odt5d2rdvn9LT09W8efNi1wOg8uAxHAC3FRERoS1btujw4cM6efKk8vPzNWzYMJ06dUpxcXHatm2bDh06pI8++kiDBw+2Bp3GjRsrNzdX8+fP1/fff6+lS5c6F37//nxnzpxRQkKCTp48WeTjuZiYGLVq1Ur9+vXTjh07tHXrVg0YMEBdunRR+/btS30OAJQ9whIAtzV27Fh5enqqefPmCg4O1tGjRxUWFqYvvvhCeXl5uuOOO9SqVSuNGjVKQUFBzjtGRWndurVmzZqlGTNmqGXLlnrzzTc1ffp0lz6dOnXSo48+qj59+ig4OLjQAnHpt8dpq1evVo0aNfSnP/1JMTExuvbaa7V8+fJSv34A5cNhjDEVXQQAAEBlxZ0lAAAAC8ISAACABWEJAADAgrAEAABgQVgCAACwICwBAABYEJYAAAAsCEsAAAAWhCUAAAALwhIAAIAFYQkAAMCCsAQAAGDx/wB8pvjnTEREQAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iters = [iter * ITERS_PER_EVAL for iter in range(len(train_losses))]\n",
    "plt.plot(iters, train_losses, label='Train')\n",
    "# plt.plot(iters, val_losses, label='validation')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6UjCTMQ_N5e",
    "outputId": "56d092d1-df05-4e7d-f2d9-5deead054d7c"
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# model.eval()\n",
    "# test_edge_index = test_edge_index.to(device)\n",
    "# test_sparse_edge_index = test_sparse_edge_index.to(device)\n",
    "\n",
    "# test_loss, test_recall, test_precision, test_ndcg = evaluation(\n",
    "#             model, test_edge_index, test_sparse_edge_index, [train_edge_index, val_edge_index], K, LAMBDA)\n",
    "\n",
    "# print(f\"[test_loss: {round(test_loss, 5)}, test_recall@{K}: {round(test_recall, 5)}, test_precision@{K}: {round(test_precision, 5)}, test_ndcg@{K}: {round(test_ndcg, 5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [3:14:49<00:00, 87.89s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test_loss: 16.93605, test_recall@10: 0.01701, test_precision@10: 0.00287, test_ndcg@10: 0.0099]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "model.eval()\n",
    "test_edge_index = test_edge_index.to(device)\n",
    "test_sparse_edge_index = test_sparse_edge_index.to(device)\n",
    "\n",
    "# Obtener la pérdida (loss) separada\n",
    "users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(test_sparse_edge_index)\n",
    "edges = structured_negative_sampling(test_edge_index, contains_neg_self_loops=False)\n",
    "user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n",
    "users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
    "pos_items_emb_final, pos_items_emb_0 = items_emb_final[pos_item_indices], items_emb_0[pos_item_indices]\n",
    "neg_items_emb_final, neg_items_emb_0 = items_emb_final[neg_item_indices], items_emb_0[neg_item_indices]\n",
    "\n",
    "test_loss = bpr_loss(\n",
    "    users_emb_final,\n",
    "    users_emb_0,\n",
    "    pos_items_emb_final,\n",
    "    pos_items_emb_0,\n",
    "    neg_items_emb_final,\n",
    "    neg_items_emb_0,\n",
    "    LAMBDA\n",
    ").item()\n",
    "\n",
    "# Calcular métricas en batches\n",
    "test_recall, test_precision, test_ndcg = get_metrics_batchwise(\n",
    "    model, \n",
    "    test_edge_index, \n",
    "    [train_edge_index, val_edge_index], \n",
    "    K, \n",
    "    batch_size=2048  # Ajusta este valor según tu memoria disponible\n",
    ")\n",
    "\n",
    "print(f\"[test_loss: {round(test_loss, 5)}, test_recall@{K}: {round(test_recall, 5)}, test_precision@{K}: {round(test_precision, 5)}, test_ndcg@{K}: {round(test_ndcg, 5)}]\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
